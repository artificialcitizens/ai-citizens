export const langgraphDocs =
  '# Common Agentic Patterns\n\n## Structured Output\n\nIt\'s pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state.\n\nSince LangGraph nodes can be arbitrary JavaScript/TypeScript functions, you can do this however you want. If you want to use LangChain, [this how-to guide](https://js.langchain.com/v0.2/docs/how_to/structured_output/) is a starting point.\n\n## Tool calling\n\nIt\'s extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools.\n\nSince LangGraph nodes can be arbitrary JavaScript/TypeScript functions, you can do this however you want. If you want to use LangChain, [this how-to guide](https://js.langchain.com/v0.2/docs/how_to/tool_calling/) is a starting point.\n\n## Memory\n\nMemory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation.\n\nLangGraph is perfectly suited to give you full control over the memory of your application. With user defined [`State`](./low_level.md#state) you can specify the exact schema of the memory you want to retain. With [checkpointers](./low_level.md#checkpointer) you can store checkpoints of previous interactions and resume from there in follow up interactions.\n\nSee [this guide](../how-tos/persistence.ipynb) for how to add memory to your graph.\n\n## Human-in-the-loop\n\nAgentic systems often require some human-in-the-loop (or "on-the-loop") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to [checkpointers](./low_level.md#checkpointer). The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to "continue" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that.\n\nThere are a few common human-in-the-loop interaction patterns we see emerging.\n\n### Approval\n\nA basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a [breakpoint](./low_level.md#breakpoints) before specific nodes.\n\nSee [this guide](../how-tos/breakpoints.ipynb) for how do this in LangGraph.\n\n### Wait for input\n\nA similar one is to have the agent wait for human input. This can be done by:\n\n1. Create a node specifically for human input\n2. Add a breakpoint before the node\n3. Get user input\n4. Update the state with that user input, acting as that node\n5. Resume execution\n\nSee [this guide](../how-tos/wait-user-input.ipynb) for how do this in LangGraph.\n\n### Edit agent actions\n\nThis is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent\'s previous decisions. This can be done either during the flow (after a [breakpoint](./low_level.md#breakpoints), part of the [approval](#approval) flow) or after the fact (as part of [time-travel](#time-travel))\n\nSee [this guide](../how-tos/edit-graph-state.ipynb) for how do this in LangGraph.\n\n### Time travel\n\nThis is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally [edit it](#edit-agent-actions), and then resume execution from there.\n\nSee [this guide](../how-tos/time-travel.ipynb) for how to do this in LangGraph.\n\n## Multi-agent\n\nA term you may have heard is "multi-agent" architectures. What exactly does this mean?\n\nGiven that it is hard to even define an "agent", it\'s almost impossible to exactly define a "multi-agent" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a [ReAct agent](#react-agent).\n\nThe big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).\n\n## Planning\n\nOne of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.\n\n## Reflection\n\nAgents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn\'t - then you can take the feedback on why it\'s not correct and pass it back into another iteration of the agent.\n\nThis "reflection" step often uses an LLM, but doesn\'t have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.\n\n## ReAct Agent\n\nOne of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools.\n\nOne of the few high level, pre-built agents we have in LangGraph - you can use it with [`createReactAgent`](https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html)\n\nThis is named after and based on the [ReAct](https://arxiv.org/abs/2210.03629) paper. However, there are several differences between this paper and our implementation:\n\n- First, we use [tool-calling](#tool-calling) to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.\n- Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn\'t even expose a message-based interface, whereas now that\'s the only interface they expose.\n- Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.\n- Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.\n- Finally, the paper asked the LLM to explicitly generate a "Thought" step before deciding which tools to call. This is the "Reasoning" part of "ReAct". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.\n\nSee [this guide](../how-tos/time-travel.ipynb) for a full walkthrough of how to use the prebuilt ReAct agent.\n\n\n-----------------------------\nFile: concepts/faq.md\n-----------------------------\n\n# FAQ\n\nCommon questions and their answers!\n\n## Do I need to use LangChain in order to use LangGraph?\n\nNo! LangGraph is a general-purpose framework - the nodes and edges are nothing more than JavaScript/TypeScript functions. You can use LangChain, raw HTTP requests, or even other frameworks inside these nodes and edges.\n\n## Does LangGraph work with LLMs that don\'t support tool calling?\n\nYes! You can use LangGraph with any LLMs. The main reason we use LLMs that support tool calling is that this is often the most convenient way to have the LLM make its decision about what to do. If your LLM does not support tool calling, you can still use it - you just need to write a bit of logic to convert the raw LLM string response to a decision about what to do.\n\n## Does LangGraph work with OSS LLMs?\n\nYes! LangGraph is totally ambivalent to what LLMs are used under the hood. The main reason we use closed LLMs in most of the tutorials is that they seamlessly support tool calling, while OSS LLMs often don\'t. But tool calling is not necessary (see [this section](#does-langgraph-work-with-llms-that-dont-support-tool-calling)) so you can totally use LangGraph with OSS LLMs.\n\n\n-----------------------------\nFile: concepts/high_level.md\n-----------------------------\n\n# LangGraph for Agentic Applications\n\n## What does it mean to be agentic?\n\nOther people may talk about a system being an "agent" - we prefer to talk about systems being "agentic". But what does this actually mean?\n\nWhen we talk about systems being "agentic", we are talking about systems that use an LLM to decide the control flow of an application. There are different levels that an LLM can be used to decide the control flow, and this spectrum of "agentic" makes more sense to us than defining an arbitrary cutoff for what is or isn\'t an agent.\n\nExamples of using an LLM to decide the control of an application:\n\n- Using an LLM to route between two potential paths\n- Using an LLM to decide which of many tools to call\n- Using an LLM to decide whether the generated answer is sufficient or more work is need\n\nThe more times these types of decisions are made inside an application, the more agentic it is.\nIf these decisions are being made in a loop, then its even more agentic!\n\nThere are other concepts often associated with being agentic, but we would argue these are a by-product of the above definition:\n\n- [Tool calling](agentic_concepts.md#tool-calling): this is often how LLMs make decisions\n- Action taking: often times, the LLMs\' outputs are used as the input to an action\n- [Memory](agentic_concepts.md#memory): reliable systems need to have knowledge of things that occurred\n- [Planning](agentic_concepts.md#planning): planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\n## Why LangGraph?\n\nLangGraph has several core principles that we believe make it the most suitable framework for building agentic applications:\n\n- [Controllability](../how-tos/index.md#controllability)\n- [Human-in-the-Loop](../how-tos/index.md#human-in-the-loop)\n- [Streaming First](../how-tos/index.md#streaming)\n\n**Controllability**\n\nLangGraph is extremely low level. This gives you a high degree of control over what the system you are building actually does. We believe this is important because it is still hard to get agentic systems to work reliably, and we\'ve seen that the more control you exercise over them, the more likely it is that they will "work".\n\n**Human-in-the-Loop**\n\nLangGraph comes with a built-in persistence layer as a first-class concept. This enables several different human-in-the-loop interaction patterns. We believe that "Human-Agent Interaction" patterns will be the new "Human-Computer Interaction", and have built LangGraph with built in persistence to enable this.\n\n**Streaming First**\n\nLangGraph comes with first class support for streaming. Agentic applications often take a while to run, and so giving the user some idea of what is happening is important, and streaming is a great way to do that. LangGraph supports streaming of both events ([like a tool call being taken](../how-tos/stream-updates.ipynb)) as well as of tokens that an LLM may emit.\n\n## Deployment\n\nSo you\'ve built your LangGraph object - now what?\n\nNow you need to deploy it. \nThere are many ways to deploy LangGraph objects, and the right solution depends on your needs and use case.\nWe\'re working on adding JavaScript/TypeScript support for LangGraph cloud, but in the meantime, here are some options:\n\n- Use [Express.js](https://expressjs.com/) to stand up a server. You can then call this graph from inside the Express.js server as you see fit.\n\n-----------------------------\nFile: concepts/low_level.md\n-----------------------------\n\n# Low Level Conceptual Guide\n\n## Graphs\n\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\n\n1. [`State`](#state): A shared data structure that represents the current snapshot of your application. It is represented by a [`channels`](https://langchain-ai.github.io/langgraphjs/reference/interfaces/index.StateGraphArgs.html#channels-1) object, and an optional TypeScript `interface`.\n\n2. [`Nodes`](#nodes): JavaScript/TypeScript functions that encode the logic of your agents. They receive the current `State` as input, perform some computation or side-effect, and return an updated `State`.\n\n3. [`Edges`](#edges): JavaScript/TypeScript functions that determine which `Node` to execute next based on the current `State`. They can be conditional branches or fixed transitions.\n\nBy composing `Nodes` and `Edges`, you can create complex, looping workflows that evolve the `State` over time. The real power, though, comes from how LangGraph manages that `State`. To emphasize: `Nodes` and `Edges` are nothing more than JavaScript/TypeScript functions - they can contain an LLM or just good ol\' JavaScript/TypeScript code.\n\nIn short: _nodes do the work. edges tell what to do next_.\n\nLangGraph\'s underlying graph algorithm uses [message passing](https://en.wikipedia.org/wiki/Message_passing) to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google\'s [Pregel](https://research.google/pubs/pregel-a-system-for-large-scale-graph-processing/) system, the program proceeds in discrete "super-steps."\n\nA super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an `inactive` state. A node becomes `active` when it receives a new message (state) on any of its incoming edges (or "channels"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to `halt` by marking themselves as `inactive`. The graph execution terminates when all nodes are `inactive` and no messages are in transit.\n\n### StateGraph\n\nThe `StateGraph` class is the main graph class to uses. This is parameterized by a user defined `State` object. (passed via the `channels` argument)\n\n\n### MessageGraph\n\nThe `MessageGraph` class is a special type of graph. The `State` of a `MessageGraph` is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the `State` to be more complex than a list of messages.\n\n### Compiling your graph\n\nTo build your graph, you first define the [state](#state), you then add [nodes](#nodes) and [edges](#edges), and then you compile it. What exactly is compiling your graph and why is it needed?\n\nCompiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like [checkpointers](#checkpointer) and [breakpoints](#breakpoints). You compile your graph by just calling the `.compile` method:\n\n```typescript\nconst graph = graphBuilder.compile(...);\n```\n\nYou **MUST** compile your graph before you can use it.\n\n## State\n\nThe first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and should be defined using the [`channels`](https://langchain-ai.github.io/langgraphjs/reference/interfaces/index.StateGraphArgs.html#channels-1) schema. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function.\n\n### Schema\n\nThe way to specify the schema of a graph is by providing a series of (`channels`)[https://langchain-ai.github.io/langgraphjs/reference/interfaces/index.StateGraphArgs.html#channels-1].\n\n### Reducers\n\nReducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. Let\'s take a look at a few examples to understand them better.\n\n**Example A:**\n\n```typescript\nimport { StateGraph } from "@langchain/langgraph";\n\ninterface State {\n  foo: number;\n  bar: string[];\n}\n\nconst graphBuilder = new StateGraph<State>({\n  channels: {\n    foo: null,\n    bar: null,\n  }\n});\n```\n\nIn this example, no reducer functions are specified for any key. Let\'s assume the input to the graph is `{ foo: 1, bar: ["hi"] }`. Let\'s then assume the first `Node` returns `{ foo: 2 }`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{ foo: 2, bar: ["hi"] }`. If the second node returns `{ bar: ["bye"] }` then the `State` would then be `{ foo: 2, bar: ["bye"] }`\n\n**Example B:**\n\n```typescript\nimport { StateGraph } from "@langchain/langgraph";\n\ninterface State {\n  foo: number;\n  bar: string[];\n}\n\nconst graphBuilder = new StateGraph<State>({\n  channels: {\n    foo: null,\n    bar: {\n      reducer: (state: string[], update: string[]) => state.concat(update),\n      default: () => [],\n    },\n  }\n});\n```\n\nIn this example, we\'ve updated our `bar` field to be an object containing a `reducer` function. This function will always accept two positional arguments: `state` and `update`, with `state` representing the current state value, and `update` representing the update returned from a `Node`. Note that the first key remains unchanged. Let\'s assume the input to the graph is `{ foo: 1, bar: ["hi"] }`. Let\'s then assume the first `Node` returns `{ foo: 2 }`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{ foo: 2, bar: ["hi"] }`. If the second node returns`{ bar: ["bye"] }` then the `State` would then be `{ foo: 2, bar: ["hi", "bye"] }`. Notice here that the `bar` key is updated by concatenating the two arrays together.\n\n### MessageState\n\n`MessageState` is one of the few opinionated components in LangGraph. `MessageState` is a special state designed to make it easy to use a list of messages as a key in your state. Specifically, `MessageState` is defined as:\n\n```typescript\nimport { BaseMessage } from "@langchain/langgraph";\n\ninterface MessageState {\n  messages: BaseMessage[];\n}\n\nexport class MessageGraph extends StateGraph {\n  constructor() {\n    super({\n      channels: {\n        __root__: {\n          reducer: messagesStateReducer,\n          default: () => [],\n        },\n      },\n    });\n  }\n}\n```\n\nWhat this is doing is creating a `State` with a single key `messages`. This is a list of `BaseMessage`s, with [`messagesStateReducer`](https://langchain-ai.github.io/langgraphjs/reference/functions/index.messagesStateReducer.html) as a reducer. `messagesStateReducer` basically adds messages to the existing list (it also does some nice extra things, like convert from OpenAI message format to the standard LangChain message format, handle updates based on message IDs, etc).\n\nWe often see a list of messages being a key component of state, so this prebuilt state is intended to make it easy to use messages. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\n\n```typescript\ninterface State extends MessagesState {\n  documents: Array<string>\n}\n```\n\n## Nodes\n\nIn LangGraph, nodes are typically JavaScript/TypeScript functions (sync or `async`) where the **first** positional argument is the [state](#state), and (optionally), the **second** positional argument is a "config", containing optional [configurable parameters](#configuration) (such as a `thread_id`).\n\nSimilar to `NetworkX`, you add these nodes to a graph using the [addNode](https://langchain-ai.github.io/langgraphjs/reference/classes/index.StateGraph.html#addNode) method:\n\n```typescript\nimport { RunnableConfig } from "@langchain/core/runnables";\nimport { StateGraph } from "@langchain/langgraph";\n\ninterface State {\n  input: string;\n  results: string;\n}\n\nconst myNode = (state: State, config?: RunnableConfig) => {\n  console.log("In node: ", config["configurable"]["user_id"])\n  return {\n    results: `Hello, ${state.input}!`\n  }  \n}\n\n\n// The second argument is optional\nconst myOtherNode = (state: State) => {\n  return state\n}\n\nconst builder = new StateGraph<State>({\n  channels: {\n    input: null,\n    results: null,\n  },\n}).addNode("myNode", myNode)\n  .addNode("myOtherNode", myOtherNode)\n  ...\n```\n\nBehind the scenes, functions are converted to [RunnableLambda\'s](https://v02.api.js.langchain.com/classes/langchain_core_runnables.RunnableLambda.html), which adds batch and streaming support to your function, along with native tracing and debugging.\n\n### `START` Node\n\nThe `START` Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\n\n```typescript\nimport { START } from "@langchain/langgraph";\n\ngraph.addEdge(START, "nodeA");\n```\n\n### `END` Node\n\nThe `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\n\n```typescript\nimport { END } from "@langchain/langgraph";\n\ngraph.addEdge("nodeA", END);\n```\n\n## Edges\n\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\n\n- Normal Edges: Go directly from one node to the next.\n- Conditional Edges: Call a function to determine which node(s) to go to next.\n- Entry Point: Which node to call first when user input arrives.\n- Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\n\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.\n\n### Normal Edges\n\nIf you **always** want to go from node A to node B, you can use the [addEdge](https://langchain-ai.github.io/langgraphjs/reference/classes/index.StateGraph.html#addEdge) method directly.\n\n```typescript\ngraph.addEdge("nodeA", "nodeB");\n```\n\n### Conditional Edges\n\nIf you want to **optionally** route to 1 or more edges (or optionally terminate), you can use the [addConditionalEdges](https://langchain-ai.github.io/langgraphjs/reference/classes/index.StateGraph.html#addConditionalEdges) method. This method accepts the name of a node and a "routing function" to call after that node is executed:\n\n```typescript\ngraph.addConditionalEdges("nodeA", routingFunction);\n```\n\nSimilar to nodes, the `routingFunction` accept the current `state` of the graph and return a value.\n\nBy default, the return value `routingFunction` is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\n\nYou can optionally provide an object that maps the `routingFunction`\'s output to the name of the next node.\n\n```typescript\ngraph.addConditionalEdges("nodeA", routingFunction, {\n  true: "nodeB",\n  false: "nodeC"\n});\n```\n\n### Entry Point\n\nThe entry point is the first node(s) that are run when the graph starts. You can use the [`addEdge`](https://langchain-ai.github.io/langgraphjs/reference/classes/index.StateGraph.html#addEdge) method from the virtual [`START`](https://langchain-ai.github.io/langgraphjs/reference/variables/index.START.html) node to the first node to execute to specify where to enter the graph.\n\n```typescript\nimport { START } from "@langchain/langgraph" \n\ngraph.addEdge(START, "nodeA")\n```\n\n### Conditional Entry Point\n\nA conditional entry point lets you start at different nodes depending on custom logic. You can use [`addConditionalEdges`](https://langchain-ai.github.io/langgraphjs/reference/classes/index.StateGraph.html#addConditionalEdges) from the virtual [`START`](https://langchain-ai.github.io/langgraphjs/reference/variables/index.START.html) node to accomplish this.\n\n```typescript\nimport { START } from "@langchain/langgraph" \n\ngraph.addConditionalEdges(START, routingFunction)\n```\n\nYou can optionally provide an object that maps the `routingFunction`\'s output to the name of the next node.\n\n```typescript\ngraph.addConditionalEdges(START, routingFunction, {\n  true: "nodeB",\n  false: "nodeC"\n});\n```\n\n## Checkpointer\n\nLangGraph has a built-in persistence layer, implemented through [checkpointers](https://langchain-ai.github.io/langgraphjs/reference/classes/index.BaseCheckpointSaver.html). When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph\'s state. The checkpointer saves a _checkpoint_ of the graph state at every super-step, enabling several powerful capabilities:\n\nFirst, checkpointers facilitate [human-in-the-loop workflows](agentic_concepts.md#human-in-the-loop) workflows by allowing humans to inspect, interrupt, and approve steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state.\n\nSecond, it allows for ["memory"](agentic_concepts.md#memory) between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones.\n\nSee [this guide](../how-tos/persistence.ipynb) for how to add a checkpointer to your graph.\n\n## Threads\n\nThreads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a `thread_id` when running the graph.\n\n`thread_id` is simply the ID of a thread. This is always required\n\nYou must pass these when invoking the graph as part of the configurable part of the config.\n\n```typescript\nconst config = { configurable: { thread_id: "a" }};\nawait graph.invoke(inputs, config);\n```\n\nSee [this guide](../how-tos/persistence.ipynb) for how to use threads.\n\n## Checkpointer state\n\nWhen interacting with the checkpointer state, you must specify a [thread identifier](#threads). Each checkpoint saved by the checkpointer has two properties:\n\n- **values**: This is the value of the state at this point in time.\n- **next**: This is a tuple of the nodes to execute next in the graph.\n\n### Get state\n\nYou can get the state of a checkpointer by calling `await graph.getState(config)`. The config should contain `thread_id`, and the state will be fetched for that thread.\n\n### Get state history\n\nYou can also call `await graph.getStateHistory(config)` to get a list of the history of the graph. The config should contain `thread_id`, and the state history will be fetched for that thread.\n\n### Update state\n\nYou can also interact with the state directly and update it. This takes three different components:\n\n- `config`\n- `values`\n- `asNode`\n\n**config**\n\nThe config should contain `thread_id` specifying which thread to update.\n\n**values**\n\nThese are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the [reducer](#reducers) functions that are part of the state. So this does NOT automatically overwrite the state. Let\'s walk through an example.\n\nLet\'s assume you have defined the state of your graph as:\n\n```typescript\ninterface State {\n  foo: number;\n  bar: string[];\n}\n\nconst channels = {\n  foo: null,\n  bar: {\n    reducer: (state: string[], update: string[]) => state.concat(update),\n    default: () => [],\n  },\n}\n```\n\nLet\'s now assume the current state of the graph is\n\n```\n{ foo: 1, bar: ["a"] }\n```\n\nIf you update the state as below:\n\n```typescript\nawait graph.updateState(config, { foo: 2, bar: ["b"] })\n```\n\nThen the new state of the graph will be:\n\n```\n{ foo: 2, bar: ["a", "b] }\n```\n\nThe `foo` key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the `bar` key, and so it appends `"b"` to the state of `bar`.\n\n**`asNode`**\n\nThe final thing you specify when calling `updateState` is `asNode`. This update will be applied as if it came from node `asNode`. If `asNode` is not provided, it will be set to the last node that updated the state, if not ambiguous.\n\nThe reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.\n\n## Graph Migrations\n\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.\n\n- For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\n- For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.\n- For modifying state, we have full backwards and forwards compatibility for adding and removing keys\n- State keys that are renamed lose their saved state in existing threads\n- State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.\n\n## Configuration\n\nWhen creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single "cognitive architecture" (the graph) but have multiple different instance of it.\n\nYou can then pass this configuration into the graph using the `configurable` config field.\n\n```typescript\nconst config = { configurable: { llm: "anthropic" }};\n\nawait graph.invoke(inputs, config);\n```\n\nYou can then access and use this configuration inside a node:\n\n```typescript\nconst nodeA = (state, config) => {\n  const llmType = config?.configurable?.llm;\n  let llm: BaseChatModel;\n  if (llmType) {\n    const llm = getLlm(llmType);\n  }\n  ...\n};\n    \n```\n\nSee [this guide](../how-tos/configuration.ipynb) for a full breakdown on configuration\n\n## Breakpoints\n\nIt can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ["compile" a graph](#compiling-your-graph). You can set breakpoints either _before_ a node executes (using `interruptBefore`) or after a node executes (using `interruptAfter`.)\n\nYou **MUST** use a [checkpoiner](#checkpointer) when using breakpoints. This is because your graph needs to be able to resume execution.\n\nIn order to resume execution, you can just invoke your graph with `null` as the input.\n\n```typescript\n// Initial run of graph\nawait graph.invoke(inputs, config);\n\n// Let\'s assume it hit a breakpoint somewhere, you can then resume by passing in None\nawait graph.invoke(null, config);\n```\n\nSee [this guide](../how-tos/breakpoints.ipynb) for a full walkthrough of how to add breakpoints.\n\n## Visualization\n\nIt\'s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs.\n\n## Streaming\n\nLangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports:\n\n- [`"values"`](../how-tos/stream-values.ipynb): This streams the full value of the state after each step of the graph.\n- [`"updates`](../how-tos/stream-updates.ipynb): This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.\n\nIn addition, you can use the [`streamEvents`](https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html#streamEvents) method to stream back events that happen _inside_ nodes. This is useful for [streaming tokens of LLM calls](../how-tos/streaming-tokens-without-langchain.ipynb).\n\nGuidelines\n\n1. Define State Interface: Always define a clear interface for the graph state, including all necessary fields and their types.\nExplanation: This ensures type safety and helps prevent runtime errors.\n\n2. Use Appropriate Reducers: Implement custom reducers for each state field that requires complex update logic.\nExplanation: Reducers control how state updates are applied, ensuring consistent state management.\n\n3. Compile Graph Before Use: Always call the .compile() method on the graph builder before invoking the graph.\nExplanation: Compilation performs necessary checks and prepares the graph for execution.\n\n4. Use START and END Nodes: Always define entry and exit points for your graph using START and END nodes.\nExplanation: This clearly defines the flow of execution in your graph.\n\n5. Implement Error Handling: Include error handling nodes and edges in your graph to manage potential failures.\nExplanation: Proper error handling improves the robustness of your application.\n\n6. Use Conditional Edges: Implement conditional edges when the next node depends on the current state.\nExplanation: This allows for dynamic routing based on the current state of the graph.\n\n7. Use Checkpointers for Stateful Graphs: When implementing stateful graphs, always use a checkpointer.\nExplanation: Checkpointers enable persistence and human-in-the-loop workflows.\n\n8. Specify Thread ID: When using checkpointers, always specify a thread_id in the configuration.\nExplanation: Thread IDs are required for managing multiple concurrent graph executions.\n\n9. Use Typed Inputs and Outputs: Ensure that all node functions have properly typed inputs and outputs.\nExplanation: This improves code reliability and makes it easier to catch errors during development.\n\n10. Handle Asynchronous Operations: Use async/await for all asynchronous operations within nodes.\nExplanation: This ensures proper handling of asynchronous tasks and prevents race conditions.';
